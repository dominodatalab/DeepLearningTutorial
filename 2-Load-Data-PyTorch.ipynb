{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe556fd1",
   "metadata": {},
   "source": [
    "# Load Serengeti images into Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc123ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms as T\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ad6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'data'\n",
    "#to use all of the species clases read in image_labels'csv\n",
    "annotations_file = 'labels_reduced_classes.csv'\n",
    "labels = pd.read_csv(annotations_file)\n",
    "\n",
    "#shuffle dataset\n",
    "labels = labels.sample(frac=1, random_state=42)\n",
    "\n",
    "print('There are currently {} rows in the dataset'.format(labels.shape[0]))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3518100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary for species\n",
    "species = sorted(labels['question__species'].unique())\n",
    "print('There are {} unique species in this dataset'.format(len(species)))\n",
    "\n",
    "species_to_idx = dict(zip(species,range(len(species))))\n",
    "idx_to_species = {v: k for k, v in species_to_idx.items()}\n",
    "idx_to_species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2de8f",
   "metadata": {},
   "source": [
    "## PyTorch Datasets\n",
    "PyTorch provides two data primitives: `DataLoader` and `Dataset`. Datasets store the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
    "To define your own, you need to inherit from the predefined Dataset class and implement three methods:\n",
    "\n",
    "- `__init__`\n",
    "- `__len__` so that `len(dataset)` returns the size of the dataset\n",
    "- `__getitem__` such that `dataset[i]` can be used to get `i`th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf38b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom class for loading in the data to PyTorch, resizing, and creating the labels\n",
    "class SnapshotSerengetiDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, class_dict, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.class2index = class_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(image_path)\n",
    "        image = torch.mul(image, 1/255.) # scale to [0, 1]\n",
    "        label = self.class2index[self.img_labels.iloc[idx, 1]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c79eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display sample of photos with no transforms\n",
    "no_transforms = SnapshotSerengetiDataset(annotations_file, img_dir, species_to_idx)\n",
    "\n",
    "figure = plt.figure(figsize=(16, 16))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(no_transforms), size=(1,)).item()\n",
    "    img, species_label = no_transforms[sample_idx]\n",
    "    label_name = list(species_to_idx.keys())[species_label]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    figure.tight_layout()\n",
    "    plt.title(label_name)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc51878",
   "metadata": {},
   "source": [
    "## PyTorch Transforms\n",
    "\n",
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "The `torchvision.transforms` module offers several commonly-used transforms out of the box. You'll get to test these out at the end of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use resize and normalize transforms so they can be used with pre-trained networks, and convert to a tensor\n",
    "#this will be applied to both the test and validation set in later notebooks\n",
    "standard_transform = T.Compose([T.Resize(256),\n",
    "                           T.ConvertImageDtype(torch.float32),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply transforms\n",
    "transformed_dataset = SnapshotSerengetiDataset(annotations_file=annotations_file, img_dir=img_dir, class_dict=species_to_idx, transform=standard_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "transformed_loader = torch.utils.data.DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4009461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot sample of transformed images\n",
    "def imshow(dataloader, cols=3, rows=3):\n",
    "    images, species_labels = next(iter(dataloader))\n",
    "    \n",
    "    figure = plt.figure(figsize=(16, 16))\n",
    "\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        figure.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        image = images[i]\n",
    "        #use since the image is normalized:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image.permute(1, 2, 0).numpy() + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        species_label = species_labels[i]\n",
    "        plt.title(idx_to_species[species_label.item()])\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images with transforms - these are what will be used in the validation set\n",
    "imshow(transformed_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106895c6",
   "metadata": {},
   "source": [
    "# Activity \n",
    "\n",
    "#### Visualize different PyTorch transforms\n",
    "\n",
    "See this document on PyTorch transforms - https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "Here is a sample of Pytorch transforms to try out:\n",
    "\n",
    "- Normalize \n",
    "- Resize\n",
    "- Scale\n",
    "- CenterCrop\n",
    "- Pad\n",
    "- RandomCrop\n",
    "- RandomHorizontalFlip\n",
    "- RandomVerticalFlip\n",
    "- RandomResizedCrop\n",
    "- RandomSizedCrop\n",
    "- LinearTransformation\n",
    "- ColorJitter \n",
    "- RandomRotation \n",
    "- RandomAffine \n",
    "- Grayscale\n",
    "- RandomGrayscale\n",
    "- RandomPerspective\n",
    "- RandomErasing\n",
    "- GaussianBlur\n",
    "- InterpolationMode\n",
    "- RandomInvert \n",
    "- RandomPosterize\n",
    "- RandomSolarize\n",
    "- RandomAdjustSharpness\n",
    "- RandomAutocontrast\n",
    "- RandomEqualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out transforms for the training set by adding to the list below\n",
    "\n",
    "my_transforms = [T.Resize(256),\n",
    "\n",
    "                 #add transforms here by preceding the transform name with T. For example: \n",
    "                 #T.RandomRotation(30),\n",
    "                 \n",
    "                 T.ConvertImageDtype(torch.float32),\n",
    "                 T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])]\n",
    "\n",
    "train_transform = T.Compose(my_transforms)\n",
    "train_dataset = SnapshotSerengetiDataset(annotations_file=annotations_file, img_dir=img_dir, class_dict=species_to_idx, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "         \n",
    "#images with train transforms\n",
    "imshow(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
