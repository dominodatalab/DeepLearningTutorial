{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchvision.io import read_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import Lambda\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our `image_labels.csv` linking to the images in `/domino/datasets/local/serengeti-small-dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'data'\n",
    "#to use all of the species clases read in image_labels'csv\n",
    "annotations_file = 'labels_reduced_classes.csv'\n",
    "\n",
    "labels = pd.read_csv(annotations_file)\n",
    "species = sorted(labels['question__species'].unique())\n",
    "species_to_idx = dict(zip(species,range(len(species))))\n",
    "idx_to_species = {v: k for k, v in species_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the number of images to use for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples (per class) must be less than 1000\n",
    "num_samples = 200\n",
    "\n",
    "small_dfs = []\n",
    "for animal in species:\n",
    "    small_df = labels.loc[labels['question__species'] == animal]\n",
    "    small_df = small_df[['image_name','question__species']].sample(num_samples, random_state=42)\n",
    "    small_dfs.append(small_df)\n",
    "\n",
    "reduced_images = pd.concat(small_dfs, ignore_index=True)\n",
    "reduced_images.to_csv('reduced_images.csv', index=False)\n",
    "\n",
    "\n",
    "#change annotations file location\n",
    "annotations_file = 'reduced_images.csv'\n",
    "labels = pd.read_csv(annotations_file)\n",
    "\n",
    "labels['question__species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the following code to see the counts of classes and the dictionary `species_to_idx` mapping species to class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 999):\n",
    "    print(labels['question__species'].value_counts())   \n",
    "print('---\\nspecies_to_idx: ' + str(species_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Custom Dataset\n",
    "\n",
    "To define your own, you need to inherit from the predefined `Dataset` class and implement three methods:\n",
    "* `__init__`\n",
    "* `__len__` so that `len(dataset)` returns the size of the dataset\n",
    "* `__getitem__` such that `dataset[i]` can be used to get `i`th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapshotSerengetiDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, class_dict, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.class2index = class_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(image_path)\n",
    "        image = torch.mul(image, 1/255.) # scale to [0, 1]\n",
    "        label = self.class2index[self.img_labels.iloc[idx, 1]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the image transforms discussed in the `2-Load-Data-PyTorch.ipynb` notebook as well as a transform to one-hot encode the species index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([T.Resize(256),\n",
    "                             T.RandomRotation(30),\n",
    "                             T.RandomHorizontalFlip(),\n",
    "                             T.CenterCrop(224),\n",
    "                             T.ConvertImageDtype(torch.float32),\n",
    "                             T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                          std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = T.Compose([T.Resize(256),\n",
    "                           T.CenterCrop(224),\n",
    "                           T.ConvertImageDtype(torch.float32),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "target_transform = Lambda(lambda y: torch.zeros(len(species_to_idx), dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into stratified train and validation sets using scikit-learn's `train_test_split`.\n",
    "\n",
    "It's good to remember that you can still import and use functions from tools you are familiar with even if you aren't using them elsewhere in a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 42\n",
    "val_size = .1\n",
    "\n",
    "def stratified_split(annotations_file, test_size = 0.2):\n",
    "    img_labels = pd.read_csv(annotations_file)\n",
    "    indices = img_labels.index\n",
    "    labels = img_labels[['question__species']]\n",
    "    train_indices, test_indices, _, _ = train_test_split(indices, labels, stratify=labels, test_size=test_size, random_state=random_seed)\n",
    "    return train_indices, test_indices\n",
    "\n",
    "train_indices, val_indices = stratified_split(annotations_file=annotations_file, test_size=val_size)\n",
    "\n",
    "dataset_size = len(train_indices) + len(val_indices)\n",
    "\n",
    "print('Train counts\\n---')\n",
    "print(labels['question__species'].iloc[train_indices].value_counts())\n",
    "print('\\nValidation counts\\n---')\n",
    "print(labels['question__species'].iloc[val_indices].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our train and validation indices, we can generate an instance of the dataset for each (with the appropriate transforms). We also generate a random sampler based on the indices to ensure that images are shuffled across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SnapshotSerengetiDataset(annotations_file=annotations_file, img_dir=img_dir, class_dict=species_to_idx, transform=train_transform, target_transform=target_transform)\n",
    "val_dataset = SnapshotSerengetiDataset(annotations_file=annotations_file, img_dir=img_dir, class_dict=species_to_idx, transform=val_transform, target_transform=target_transform)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Pytorch dataloaders wrap an iterable around a dataset and make it easier to feed data to a model during training.\n",
    "\n",
    "The relevant parameter here is batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the tensor sizes and other intermediate data is one of the easiest ways to confirm that we are prepping the data as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out tensor sizes\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {images.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n",
    "label = labels[0]\n",
    "print(\"Label: \" + idx_to_species[label.argmax(0).item()])\n",
    "\n",
    "image = images[0]\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "image = std * image.permute(1, 2, 0).numpy() + mean\n",
    "image = np.clip(image, 0, 1)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Loops\n",
    "\n",
    "One of pytorch's greatest strengths is that you explicitly control what happens at every step of the model training.\n",
    "\n",
    "The `train_model` function shown here loops through the provided number of epochs to complete the train and validation steps at each.\n",
    "\n",
    "Depending on what you're trying to have your model do, your training function may be significantly more or less complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.argmax(1))\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a model\n",
    "\n",
    "Although it's good to understand the relevant deep learning model components (e.g. convolutional layers, ReLU) you generally don't need to build an architecture from scratch.\n",
    "\n",
    "Instead, you can find a model architecture that has already been proven to perform well on a similar problem ane start with that!\n",
    "\n",
    "To facilitate this, pytorch has a library of popular and state-of-the-art model architectures that you can import into your project. We'll use the resnet50 architecture here, because it has been shown to work well for animal recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Large models can require a lot of data and are expensive to train. One common approach to mitigate this is transfer learning.\n",
    "\n",
    "Transfer learning is using what was learned for one task to solve a different task.\n",
    "\n",
    "Here, we use pytorch's pretrained resnet50 (trained on the popular 1000-class ImageNet database). To exclude the pretrained features of the model from training we freeze all but the final layer, which maps the features to the number of expected classes. We also change the final layer to have an output neuron for each species class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(species_to_idx)) # Rescale fully-connected layer output size\n",
    "num_frozen_layers = 9\n",
    "\n",
    "# Freeze `num_frozen_layers`\n",
    "layer = 0\n",
    "for child in model.children():\n",
    "    layer += 1\n",
    "    if layer <= num_frozen_layers:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "print('Number of unfrozen layers: ' + str(layer-num_frozen_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the command below to see details about what's happening in each layer of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train!\n",
    "\n",
    "Now we're ready to train our model. We first do final formatting of our inputs and define a loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataset_sizes = {'train': dataset_size*(1-val_size), 'val': dataset_size*val_size}\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-3)\n",
    "\n",
    "model = train_model(model, loss_fn, optimizer, 1, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Output\n",
    "\n",
    "One of the most important things you can do is **look at your data!**\n",
    "\n",
    "Visualizing samples from your model with the predicted label can give you confidence that it is performing as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloaders['val']):\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            figure = plt.figure(figsize=(16, 16))\n",
    "            for j in range(num_images):\n",
    "                images_so_far += 1\n",
    "                figure.add_subplot(num_images//2, 2, images_so_far)\n",
    "                figure.tight_layout()\n",
    "                plt.axis('off')\n",
    "                image = images.cpu().data[j]\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                image = std * image.permute(1, 2, 0).numpy() + mean\n",
    "                image = np.clip(image, 0, 1)\n",
    "                plt.title('predicted: {}'.format(idx_to_species[preds[j].item()]))\n",
    "                plt.imshow(image)\n",
    "                \n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "            plt.show()\n",
    "                \n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "visualize_model(model, num_images=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
